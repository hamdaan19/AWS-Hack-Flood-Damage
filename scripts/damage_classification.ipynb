{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6a02b8-c393-4140-8444-5db1a17e14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a44637-543d-41b3-be52-75805898fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xception(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(128, 128, 3)\n",
    ")\n",
    "\n",
    "for layer in X.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d646dff1-99fd-4a54-9560-5a4620e58c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(X.output)\n",
    "X_out = Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b74102b-f005-4d82-8de7-73547b14e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "model = Model(inputs=X.input, outputs=X_out)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b7ed098-9fda-4182-8a11-72e86ee5b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.02),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834a7676-162e-4d3d-8454-96009b9f1a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting from train_another/damage/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:05<00:00, 934.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting from train_another/no_damage/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5400/5400 [00:06<00:00, 835.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples is 10361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Collecting Train Data\n",
    "\n",
    "DATA_PATH = \"../../my-datasets/Hurricane_Damage/train_another/\"\n",
    "CATEGORIES = [\"damage\", \"no_damage\"]\n",
    "\n",
    "train_data_list = []\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    dir_path = os.path.join(DATA_PATH, category)\n",
    "    img_list = os.listdir(dir_path)\n",
    "    class_num = CATEGORIES.index(category)\n",
    "    print(f\"Collecting from train_another/{category}/\")\n",
    "    for img_path in tqdm(img_list):\n",
    "        img = Image.open(os.path.join(dir_path, img_path))\n",
    "        img_arr = np.array(img, dtype=int)\n",
    "        if img_arr.shape[-1] == 4:\n",
    "            img_arr = img_arr[...,:3]\n",
    "        if img_arr.shape != (128, 128, 3):\n",
    "            continue\n",
    "        train_data_list.append([img_arr, class_num])\n",
    "        \n",
    "print(f\"Total number of samples is {len(train_data_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e37754b6-9ebf-4d60-870e-4d042bfabbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10361, 128, 128, 3)\n",
      "(10361,)\n",
      "(10361, 2)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(train_data_list)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for image, label in train_data_list:\n",
    "    X_train.append(image)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "### Converting y into a one-hot vector ###\n",
    "shape = (y_train.size, y_train.max()+1)\n",
    "y_train_one_hot = np.zeros(shape)\n",
    "rows = np.arange(y_train.size)\n",
    "y_train_one_hot[rows, y_train] = 1\n",
    "\n",
    "print(y_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cff12ff-4cff-4c96-b90a-4f32dc3d41ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting from test/damage/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1066.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting from test/no_damage/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1237.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples is 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Collecting Test Data\n",
    "\n",
    "DATA_PATH = \"../../my-datasets/Hurricane_Damage/test/\"\n",
    "CATEGORIES = [\"damage\", \"no_damage\"]\n",
    "\n",
    "test_data_list = []\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    dir_path = os.path.join(DATA_PATH, category)\n",
    "    img_list = os.listdir(dir_path)\n",
    "    class_num = CATEGORIES.index(category)\n",
    "    print(f\"Collecting from test/{category}/\")\n",
    "    for img_path in tqdm(img_list):\n",
    "        img = Image.open(os.path.join(dir_path, img_path))\n",
    "        img_arr = np.array(img)\n",
    "        test_data_list.append([img_arr, class_num])\n",
    "        \n",
    "print(f\"Total number of samples is {len(test_data_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ad7259-0d44-4925-8e90-5c1f88249bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 128, 128, 3)\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.Random(30).shuffle(test_data_list)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for image, label in test_data_list:\n",
    "    X_test.append(image)\n",
    "    y_test.append(label)\n",
    "\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.size)\n",
    "\n",
    "### Converting y_test into a one-hot vector ###\n",
    "shape = (y_test.size, y_test.max()+1)\n",
    "y_test_one_hot = np.zeros(shape)\n",
    "rows = np.arange(y_test.size)\n",
    "y_test_one_hot[rows, y_test] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f237853-0de2-4d86-80dc-911c5bec9e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 13:51:55.630533: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4074110976 exceeds 10% of free system memory.\n",
      "2022-02-03 13:51:59.572122: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4074110976 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1296/1296 [==============================] - 30s 18ms/step - loss: 233.3479 - accuracy: 0.7685 - val_loss: 234.0477 - val_accuracy: 0.7530\n",
      "Epoch 2/15\n",
      "1296/1296 [==============================] - 22s 17ms/step - loss: 229.9109 - accuracy: 0.8071 - val_loss: 142.9180 - val_accuracy: 0.8415\n",
      "Epoch 3/15\n",
      "1296/1296 [==============================] - 21s 16ms/step - loss: 197.3234 - accuracy: 0.8278 - val_loss: 147.0056 - val_accuracy: 0.8400\n",
      "Epoch 4/15\n",
      "1296/1296 [==============================] - 21s 16ms/step - loss: 196.2773 - accuracy: 0.8399 - val_loss: 461.0207 - val_accuracy: 0.7205\n",
      "Epoch 5/15\n",
      "1296/1296 [==============================] - 21s 16ms/step - loss: 176.9405 - accuracy: 0.8512 - val_loss: 308.2006 - val_accuracy: 0.7870\n",
      "Epoch 6/15\n",
      "1296/1296 [==============================] - 22s 17ms/step - loss: 160.7244 - accuracy: 0.8581 - val_loss: 126.6076 - val_accuracy: 0.8860\n",
      "Epoch 7/15\n",
      "1296/1296 [==============================] - 22s 17ms/step - loss: 158.0875 - accuracy: 0.8640 - val_loss: 366.1917 - val_accuracy: 0.7645\n",
      "Epoch 8/15\n",
      "1296/1296 [==============================] - 22s 17ms/step - loss: 143.9648 - accuracy: 0.8704 - val_loss: 622.2095 - val_accuracy: 0.6785\n",
      "Epoch 9/15\n",
      "1296/1296 [==============================] - 22s 17ms/step - loss: 181.1989 - accuracy: 0.8684 - val_loss: 136.9327 - val_accuracy: 0.8800\n",
      "Epoch 10/15\n",
      "1296/1296 [==============================] - 22s 17ms/step - loss: 148.7984 - accuracy: 0.8806 - val_loss: 162.8605 - val_accuracy: 0.8735\n",
      "Epoch 11/15\n",
      "1296/1296 [==============================] - 22s 17ms/step - loss: 130.2642 - accuracy: 0.8859 - val_loss: 144.7686 - val_accuracy: 0.8890\n",
      "Epoch 12/15\n",
      "1296/1296 [==============================] - 22s 17ms/step - loss: 155.1035 - accuracy: 0.8796 - val_loss: 202.8379 - val_accuracy: 0.8620\n",
      "Epoch 13/15\n",
      "1296/1296 [==============================] - 22s 17ms/step - loss: 123.5344 - accuracy: 0.8940 - val_loss: 436.6497 - val_accuracy: 0.7765\n",
      "Epoch 14/15\n",
      "1296/1296 [==============================] - 22s 17ms/step - loss: 127.9326 - accuracy: 0.8932 - val_loss: 777.0240 - val_accuracy: 0.6590\n",
      "Epoch 15/15\n",
      "1296/1296 [==============================] - 23s 18ms/step - loss: 116.9615 - accuracy: 0.8997 - val_loss: 150.1241 - val_accuracy: 0.8735\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_path = \"archive/Xception_model/Xception_damage_classification.ckpt\"\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    ),\n",
    "]\n",
    "\n",
    "# This line below must be commented when running this cell for the first time\n",
    "#model.load_weights(checkpoint_path)\n",
    "\n",
    "model_history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train_one_hot,\n",
    "    validation_data=(X_test, y_test_one_hot),\n",
    "    batch_size=8,\n",
    "    epochs=15,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5875eca-fe8c-4a44-8005-72e97bb76ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [233.34791564941406,\n",
       "  229.91091918945312,\n",
       "  197.32342529296875,\n",
       "  196.27734375,\n",
       "  176.9404754638672,\n",
       "  160.7244415283203,\n",
       "  158.08750915527344,\n",
       "  143.9647674560547,\n",
       "  181.1988525390625,\n",
       "  148.7984161376953,\n",
       "  130.26416015625,\n",
       "  155.103515625,\n",
       "  123.53437042236328,\n",
       "  127.93260955810547,\n",
       "  116.9615249633789],\n",
       " 'accuracy': [0.7684586644172668,\n",
       "  0.807064950466156,\n",
       "  0.82781583070755,\n",
       "  0.8398803472518921,\n",
       "  0.851172685623169,\n",
       "  0.8581218123435974,\n",
       "  0.8640092611312866,\n",
       "  0.8703793287277222,\n",
       "  0.8684489727020264,\n",
       "  0.8806099891662598,\n",
       "  0.8859183192253113,\n",
       "  0.8796448111534119,\n",
       "  0.8940256834030151,\n",
       "  0.8931570053100586,\n",
       "  0.8997201323509216],\n",
       " 'val_loss': [234.0476531982422,\n",
       "  142.9180145263672,\n",
       "  147.00559997558594,\n",
       "  461.0207214355469,\n",
       "  308.2005920410156,\n",
       "  126.60762023925781,\n",
       "  366.19171142578125,\n",
       "  622.20947265625,\n",
       "  136.9326629638672,\n",
       "  162.8605194091797,\n",
       "  144.76861572265625,\n",
       "  202.83786010742188,\n",
       "  436.649658203125,\n",
       "  777.0240478515625,\n",
       "  150.12408447265625],\n",
       " 'val_accuracy': [0.753000020980835,\n",
       "  0.8414999842643738,\n",
       "  0.8399999737739563,\n",
       "  0.7204999923706055,\n",
       "  0.7870000004768372,\n",
       "  0.8859999775886536,\n",
       "  0.7645000219345093,\n",
       "  0.6784999966621399,\n",
       "  0.8799999952316284,\n",
       "  0.8734999895095825,\n",
       "  0.8889999985694885,\n",
       "  0.8619999885559082,\n",
       "  0.7764999866485596,\n",
       "  0.6589999794960022,\n",
       "  0.8734999895095825]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c02f7-f6f3-42f2-bbc9-52c2ed00dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"archive/Xception_model/Xception_damage_classification.ckpt\"\n",
    "\n",
    "X = Xception(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(128, 128, 3)\n",
    ")\n",
    "\n",
    "x = Flatten()(X.output)\n",
    "X_out = Dense(2, activation='softmax')(x)\n",
    "new_model = Model(inputs=X.input, outputs=X_out)\n",
    "\n",
    "new_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b3b33-a62c-4385-b0cc-e57b8fcb2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../my-datasets/Hurricane_Damage/test/damage/-93.65936500000001_30.138793.jpeg\"\n",
    "test_img = Image.open(path)\n",
    "test_arr = np.asarray(test_img)\n",
    "test_arr = np.expand_dims(test_arr, axis=0) \n",
    "pred = new_model.predict(x=test_arr)\n",
    "print(pred)\n",
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2966beea-678d-4547-bba8-e40c9d00d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ = np.argmax(pred, axis=1)\n",
    "if(pred_ == 0):\n",
    "    print(\"Hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
