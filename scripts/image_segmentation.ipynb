{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c7fd9a-38cf-4adc-901a-dfdc63f53661",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading IMAGE and MASK data into python lists ###\n",
    "\n",
    "import os\n",
    "target_dir = \"/home/studio-lab-user/my-datasets/FloodNet Challenge @ EARTHVISION 2021 - Track 1/Train/Labeled\"\n",
    "image_dir = \"/home/studio-lab-user/my-datasets/FloodNet Challenge @ EARTHVISION 2021 - Track 1/Train/Classification\"\n",
    "CATEGORIES = [\"Flooded\", \"Non-Flooded\"]\n",
    "\n",
    "mask_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, category, \"mask\", mask_name)\n",
    "        for category in CATEGORIES\n",
    "        for mask_name in os.listdir(os.path.join(target_dir, category, \"mask\"))\n",
    "        if mask_name.endswith(\".png\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "image_paths = []\n",
    "for path in mask_paths:\n",
    "    if(path.find(\"Non-Flooded\") != -1): # Check if \"Non-FLooded\" is present in path\n",
    "        temp = target_dir + \"/\" + CATEGORIES[1] + \"/mask/\"\n",
    "        im_path = path[len(temp):-8]+\".jpg\"\n",
    "        full_path = os.path.join(image_dir, CATEGORIES[1], im_path)\n",
    "    else: # Otherwise \n",
    "        temp = target_dir + \"/\" + CATEGORIES[0] + \"/mask/\"\n",
    "        im_path = path[len(temp):-8] + \".jpg\"\n",
    "        full_path = os.path.join(image_dir, CATEGORIES[0], im_path)             \n",
    "    image_paths.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18940a-691e-4b2a-a198-9d22163f99d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import PIL\n",
    "from PIL import ImageOps\n",
    "\n",
    "sample_num = 12\n",
    "\n",
    "# Display input image #7\n",
    "display(Image(filename=image_paths[sample_num]))\n",
    "\n",
    "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
    "img = PIL.ImageOps.autocontrast(load_img(mask_paths[sample_num]))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a68c75c6-7c29-4b6f-bf2f-827a8684937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "'''\n",
    "Helper class to load data in batches. Uses keras.utils.Sequence. \n",
    "Refer https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "'''\n",
    "\n",
    "class FloodNetDataLoader(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, img_size, image_paths, mask_paths, img_channels=3):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "    \n",
    "    def __len__(self, ):\n",
    "        return len(self.image_paths)//self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.batch_size\n",
    "        batch_image_paths = self.image_paths[i:i+self.batch_size]\n",
    "        batch_mask_paths = self.mask_paths[i:i+self.batch_size]\n",
    "        \n",
    "        X = np.zeros((self.batch_size,) + self.img_size + (self.img_channels,), dtype=\"float32\")\n",
    "        for n, path in enumerate(batch_image_paths):\n",
    "            img = load_img(path=path, target_size=self.img_size)\n",
    "            X[n] = img\n",
    "        \n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for n, path in enumerate(batch_mask_paths):\n",
    "            mask = load_img(path=path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            y[n] = np.expand_dims(mask, axis=2) # converts (height, width) -> (height, width, 1) to add one channel as grayscale\n",
    "            \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bcd985-cfe5-4a78-be3e-f634981dce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_xception_model import get_model\n",
    "\n",
    "IMG_SIZE = (240, 320)\n",
    "\n",
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "keras.backend.clear_session() \n",
    "\n",
    "my_model = get_model(img_size=IMG_SIZE, in_channels=3, classes=10)\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5846ceee-cfef-4512-b538-5b8555fbb2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spliting data into Train set and Validation set ###\n",
    "\n",
    "import random \n",
    "\n",
    "random.Random(454).shuffle(image_paths)\n",
    "random.Random(454).shuffle(mask_paths)\n",
    "\n",
    "val_samples = 65 # number of data samples being set aside for validation\n",
    "\n",
    "train_set_images = image_paths[:-val_samples] \n",
    "train_set_masks = mask_paths[:-val_samples]\n",
    "test_set_images = image_paths[-val_samples:]\n",
    "test_set_masks = mask_paths[-val_samples:]\n",
    "\n",
    "### Loading the data ###\n",
    "\n",
    "IMG_SIZE = (240, 320)\n",
    "train_set = FloodNetDataLoader(\n",
    "    batch_size=8, img_size=IMG_SIZE, image_paths=train_set_images, mask_paths=train_set_masks\n",
    ")\n",
    "test_set = FloodNetDataLoader(\n",
    "    batch_size=8, img_size=IMG_SIZE, image_paths=test_set_images, mask_paths=test_set_masks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f182789-38b8-410e-8b7c-904a31f9ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c1953-2dda-41fc-9f42-e3b5f35e36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_path = \"archive/UNET_X/UNET_X_floodnet.ckpt\"\n",
    "\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    ),\n",
    "]\n",
    "\n",
    "my_model.load_weights(checkpoint_path)\n",
    "\n",
    "my_model.fit(\n",
    "    x=train_set,\n",
    "    epochs=1,\n",
    "    validation_data=test_set,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2f6308-2632-4faa-bcb5-119ac3e7bfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f151c57e1c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unet_xception_model import get_model\n",
    "new_model = get_model(img_size=(240, 320), in_channels=3, classes=10)\n",
    "\n",
    "checkpoint_path = \"archive/UNET_X/UNET_X_floodnet.ckpt\"\n",
    "new_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d9a8d-72f9-4b82-b518-51b4f4c437ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model.predict can also take in a numpy array of dimentions (batch_size, height, width, in_channels)\n",
    "'''\n",
    "preds = new_model.predict(test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
