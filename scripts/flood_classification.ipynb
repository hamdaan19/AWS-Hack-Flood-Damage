{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1be0e891-5ed0-4258-ad11-bbed9f54f929",
   "metadata": {},
   "source": [
    "#### Installing Packages\n",
    "\n",
    "<i>%pip install tensorflow <br>\n",
    "%pip install keras<br>\n",
    "%pip install matplotlib<br>\n",
    "%pip install pillow<br>\n",
    "%pip install tqdm<br>\n",
    "%pip install -U scikit-learn</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4296356c-8099-4e68-ae62-7e2943e2bc6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from PIL import Image, ImageFile\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73bab1ba-3143-44c8-bd16-d744f75b0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [300, 400]\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27be08f0-f0d6-4516-a3b7-961f221b7ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 300, 400, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 300, 400, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 300, 400, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 150, 200, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 150, 200, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 150, 200, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 75, 100, 128)      0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 75, 100, 256)      295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 75, 100, 256)      590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 75, 100, 256)      590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 37, 50, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 37, 50, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 37, 50, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 37, 50, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 18, 25, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 18, 25, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 18, 25, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 18, 25, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 9, 12, 512)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 55296)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 110594    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,825,282\n",
      "Trainable params: 110,594\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Flatten()(vgg.output)\n",
    "preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=vgg.input, outputs=preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "224f199a-037d-4198-9080-caccf0636ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68eddd92-da33-422c-9d26-768c695f438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [00:31<00:00,  4.19it/s]\n",
      "100%|██████████| 347/347 [01:16<00:00,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples is 480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "CLASSIFICATION_DATA_PATH = \"../../my-datasets/FloodNet Challenge @ EARTHVISION 2021 - Track 1/Train/Classification\"\n",
    "CATEGORIES = [\"Flooded\", \"Non-Flooded\"]\n",
    "data_list = []\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(CLASSIFICATION_DATA_PATH, category)\n",
    "    class_num = CATEGORIES.index(category)\n",
    "    for img_path in tqdm(os.listdir(path)):\n",
    "        try:\n",
    "            img = Image.open(os.path.join(path, img_path))\n",
    "            img = np.array(img.resize((400, 300)))\n",
    "            data_list.append([img, class_num])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "print(f\"Total number of samples is {len(data_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93349148-4503-401f-bc58-d92d69aab911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(data_list)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for image, label in data_list:\n",
    "    X.append(image)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X).reshape(-1, 300, 400, 3)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab22ff8-c4cf-4955-9049-3b99d01aefda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnp.savez_compressed(\"data/X_images\", X, allow_pickle=True)\\nnp.savez_compressed(\"data/y_labels\", y, allow_pickle=True)\\nX = np.load(\"X_images.npy\")\\ny = np.load(\"y_labels.npy\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "np.savez_compressed(\"data/X_images\", X, allow_pickle=True)\n",
    "np.savez_compressed(\"data/y_labels\", y, allow_pickle=True)\n",
    "X = np.load(\"X_images.npy\")\n",
    "y = np.load(\"y_labels.npy\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbbc29fc-a3ad-41c0-8790-1d65f8ccb382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train set: 360\n",
      "Number of samples in test set: 120\n",
      "Epoch 1/3\n",
      "23/23 [==============================] - 15s 311ms/step - loss: 4.5791 - accuracy: 0.8750\n",
      "Epoch 2/3\n",
      "23/23 [==============================] - 3s 141ms/step - loss: 0.4940 - accuracy: 0.9750\n",
      "Epoch 3/3\n",
      "23/23 [==============================] - 3s 140ms/step - loss: 8.4826e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8da5905820>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=29)\n",
    "\n",
    "print(f\"Number of samples in train set: {len(X_train)}\")\n",
    "print(f\"Number of samples in test set: {len(X_test)}\")\n",
    "\n",
    "model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=16,\n",
    "    epochs=3,\n",
    "    verbose=1,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5b6ef4c-e0c9-4d6e-8597-a3e96883228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "#model.evaluate(x=X, y=y, batch_size=16)\n",
    "preds = model.predict(x=X_test)\n",
    "preds_ = np.argmax(preds, axis=1)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for pred, truth in zip(preds_, y_test):\n",
    "    if(pred == truth):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "print(f\"Accuracy: {(correct/total)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d615d30-0c37-4d16-8295-600472ca4d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 14:49:51.805240: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_classification_model/assets\n"
     ]
    }
   ],
   "source": [
    "#model.save(\"vgg_classification_model\")\n",
    "#my_model = keras.models.load_model(\"vgg_classification_model\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff718c-19d7-4c07-85c4-8daaa65f79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = keras.models.load_model(\"vgg_classification_model\")\n",
    "\n",
    "prediction = my_model.predict(x="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
